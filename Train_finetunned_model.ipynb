{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPg42zRzdInIs0iOgpd8naU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankritRisal/Finetuning_LLM/blob/main/Train_finetunned_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Play with data\n",
        "\n"
      ],
      "metadata": {
        "id": "cnuW71GTNW6m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiKbjTf5NBO-"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "PQDmCG6KN5vD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Get the token from Colab secrets\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "login(token=hf_token)\n"
      ],
      "metadata": {
        "id": "Yb2-S9QjcviM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side =\"left\")\n",
        "tokenizer.pad_token = tokenizer.eos_token # padding tokens to make of same shape\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                             dtype = torch.bfloat16,\n",
        "                                             device_map = device)"
      ],
      "metadata": {
        "id": "Rw2uACxhfzmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization of prediction"
      ],
      "metadata": {
        "id": "F-6Be35wuRsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WHAT DO BATCH PROMPT SHOULD INCLUDE ? => CHAT_TEMPLATE + \" \" + CATEGORY_TEMPLATE\n",
        "# CATEGORY TEMPLATE => TITLE AND DESCRIPTION WITH VALID CATEGORY AS ANSWER (SHOULD OPERATE IN LOOP)\n",
        "#   SYSTEM PROMPT , PORT PROMPT > CATEGORY TEMPLATE"
      ],
      "metadata": {
        "id": "jF0W53R3zBdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/X3s4c5/FinetunningLLMmodels/book_description.csv\")\n",
        "df.drop([\"Unnamed: 0\", \"Unnamed: 0.1\", \"Price\", \"Avilability\", \"Stars\", ], axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "UHypglYKyuyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "KfyH4ncpjn3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Category\"].unique()"
      ],
      "metadata": {
        "id": "tgrNddeaivgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Category\"].value_counts()"
      ],
      "metadata": {
        "id": "FeFyC8euqT7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[~df[\"Category\"].isin([\"Add a comment\", \"Default\"])]"
      ],
      "metadata": {
        "id": "iYJ5By8tr4Ae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered = df[df[\"Category\"] == \"Add a comment\"]\n",
        "print(filtered)"
      ],
      "metadata": {
        "id": "sQ1jGiNer7qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_col = \"Category\"\n",
        "counts = df[label_col].value_counts()\n",
        "valid_classes = counts[counts >= 10].index\n",
        "df_filtered = df[df[label_col].isin(valid_classes)]"
      ],
      "metadata": {
        "id": "tA_GU2xuq-wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sampled = (\n",
        "    df_filtered\n",
        "    .groupby(label_col, group_keys=False)\n",
        "    .apply(lambda x: x.sample(n=5, random_state=42))\n",
        "    .reset_index(drop=True)\n",
        ")\n"
      ],
      "metadata": {
        "id": "QGmZoUQVrEbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df_sampled\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "q-frcP7ErS4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counts = df[label_col].value_counts()\n",
        "counts"
      ],
      "metadata": {
        "id": "gUj9S5oLrZJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find out irregular terms in book_description\n",
        "filtered = df[df[\"Book_Description\"] == \"\\n\\n\\n\\n\\n\\n\"]\n",
        "print(filtered)"
      ],
      "metadata": {
        "id": "8kC1WNMmsfgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "short_rows = df[df[\"Book_Description\"].str.len().fillna(0) < 20]\n",
        "short_rows"
      ],
      "metadata": {
        "id": "tFU1T2-StKUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build new df column to store prompt and tokenize using chat_template\n",
        "def build_prompt(row):\n",
        "    valid_category = list(df[\"Category\"].unique())\n",
        "    SYSTEM_PROMPT =  \\\n",
        "      {\n",
        "        \"role\" : \"system\",\n",
        "        \"content\" : f\"\"\" You are an AI system that reads an Title and Book Description and classifies category of the book applied, you must\n",
        "        choose from the following classes:\n",
        "        {\"\\n or \". join([\"Labeled Category:\" + x for x in list(valid_category)])}.\n",
        "        Ensure Output is from above list only\"\"\"\n",
        "        }\n",
        "\n",
        "    ASSISTANT_MESSAGES = \\\n",
        "      {\n",
        "        \"role\" : \"assistant\",\n",
        "        \"content\" : \"Labeled Category :\"\n",
        "        }\n",
        "\n",
        "    USER_MESSAGES = {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"\"\"Title: {row['Title']}, Description: {row['Book_Description']}\"\"\"\n",
        "    }\n",
        "\n",
        "    prompt = [\n",
        "        SYSTEM_PROMPT,\n",
        "        USER_MESSAGES,\n",
        "        ASSISTANT_MESSAGES\n",
        "    ]\n",
        "    # print(prompt)\n",
        "    tokenized_prompt = tokenizer.apply_chat_template(prompt, continue_final_message= True, tokenize= False)\n",
        "    return tokenized_prompt"
      ],
      "metadata": {
        "id": "nPNPGJIGoPUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Seperation into Train and Test"
      ],
      "metadata": {
        "id": "lwFSHEeuurrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_seed = 32\n",
        "train_size = 0.80\n",
        "df[\"prompt\"] = df.apply(build_prompt, axis=1)\n",
        "df = df.sample(frac=1, random_state = random_seed).reset_index(drop=True).reset_index()\n",
        "train_len = int(train_size * len(df))\n",
        "df_train= df[:train_len]\n",
        "df_test = df[train_len:]"
      ],
      "metadata": {
        "id": "H5uzUcDSoaQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train"
      ],
      "metadata": {
        "id": "XjKlRNWGufuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test"
      ],
      "metadata": {
        "id": "F-waz3FBuhvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataLoader"
      ],
      "metadata": {
        "id": "qRBe_vq7wjF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset #defined by huggingface not pandas or tf\n",
        "batch_size = 2\n",
        "train_dataset = Dataset.from_pandas(df_train)\n",
        "test_dataset = Dataset.from_pandas(df_test)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size = batch_size, shuffle= False)"
      ],
      "metadata": {
        "id": "fJNnuY-owea5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "8upsSjAYzGy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_input_output_pair(batch_prompts, batch_targets):\n",
        "  \"\"\" Pass data batch prompt ie: df[\"prompt\"]. along with the  target value ie: df[\"Category\"]\n",
        "  Now this is passed through tokenizer and padding processes\n",
        "  \"\"\"\n",
        "  full_response_text = [\n",
        "      (b_prompt + \" \" + target + tokenizer.eos_token)\n",
        "      for b_prompt, target in zip(batch_prompts, batch_targets)\n",
        "      ]\n",
        "  input_ids_tokenized = tokenizer(full_response_text, add_special_tokens = False, return_tensors =\"pt\", padding =True)[\"input_ids\"]\n",
        "  label_tokenized = tokenizer([\" \" + target + tokenizer.eos_token for target in batch_targets], add_special_tokens = False,\n",
        "                              return_tensors =\"pt\", padding = \"max_length\", max_length = input_ids_tokenized.shape[1])[\"input_ids\"]\n",
        "\n",
        "  label_tokenized_fixed = torch.where(label_tokenized != tokenizer.pad_token_id, label_tokenized, -100)\n",
        "  label_tokenized_fixed[:, -1] = tokenizer.eos_token_id\n",
        "\n",
        "  input_ids_tokenized_left_shifted = input_ids_tokenized[:, :-1]\n",
        "  label_tokenized_right_shifted = label_tokenized_fixed[:, 1:]\n",
        "\n",
        "  attention_mask = input_ids_tokenized_left_shifted != tokenizer.pad_token_id\n",
        "\n",
        "  return {\n",
        "      \"input_ids\" : input_ids_tokenized_left_shifted,\n",
        "      \"attention_mask\" : attention_mask,\n",
        "      \"labels\" : label_tokenized_right_shifted\n",
        "  }"
      ],
      "metadata": {
        "id": "bzt9JcoG9UtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "\n",
        "def calculate_loss(logits, labels):\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  entropyloss = loss_fn(logits.view(-1, logits.size(-1)), labels.reshape(-1)) # based on the tensor input, tensors are reshape to match broadcasting issues\n",
        "  return entropyloss"
      ],
      "metadata": {
        "id": "8KXdyjoBDKJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LORA ADAPTOR**"
      ],
      "metadata": {
        "id": "Hfo5B35cIrWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.max_memory_allocated() / 1e9"
      ],
      "metadata": {
        "id": "8iwv2yQcFjhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LORA ADAPTOR\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    task_type = \"CAUSAL_LM\",\n",
        "    r = 4,\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0.05,\n",
        "    target_modules = ['q_proj', 'v_proj']\n",
        ")\n",
        "\n",
        "peft_model = get_peft_model(model, lora_config)\n",
        "peft_model.print_trainable_parameters()\n",
        "model.gradient_checkpointing_enable()"
      ],
      "metadata": {
        "id": "CHWvVt_4IJEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## DONE : in order to maintain forward tensor and recomputation tensor numbers\n",
        "\n",
        "# Check if gradient checkpointing is enabled\n",
        "print(f\"Gradient checkpointing enabled: {model.is_gradient_checkpointing}\")\n",
        "\n",
        "# Or check the base model\n",
        "if hasattr(model.base_model, 'gradient_checkpointing'):\n",
        "    print(f\"Base model checkpointing: {model.base_model.gradient_checkpointing}\")"
      ],
      "metadata": {
        "id": "zjLj-9nF4MIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DEFINE TRAINING LOOP**"
      ],
      "metadata": {
        "id": "1AjxDPw6z8cX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 4\n",
        "optimizer = AdamW(model.parameters(), lr = 1e-3, weight_decay= 0.01)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  running_loss = 0.0\n",
        "  count = 0\n",
        "\n",
        "  for batch in train_dataloader :\n",
        "    data = generate_input_output_pair(batch_prompts= batch['prompt'], batch_targets= batch['Category'])\n",
        "    out = model(input_ids = data[\"input_ids\"].to(device))\n",
        "    loss = calculate_loss(out.logits, data[\"labels\"].to(device))\n",
        "    count += 1\n",
        "    print(f\"count/epoch: {count}/{epoch}\")\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    running_loss += loss.item()\n",
        "\n",
        "  avg_loss = running_loss / len(train_dataloader)\n",
        "  print(f\"avg_loss: {avg_loss}, running_loss : {running_loss}\")"
      ],
      "metadata": {
        "id": "wBtzyvP-CY7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "em__x9NP-B85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing proportion"
      ],
      "metadata": {
        "id": "HQycWe58zAcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_labels_from_output(decode_batch, tokenizer):\n",
        "  labels = []\n",
        "  for d in decode_batch:\n",
        "    # print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "    # print(d)\n",
        "    label = d.split(\"Labeled Category :\")[1].strip()\n",
        "    # print(label)\n",
        "    labels.append(label)\n",
        "  # labels = [d.split(\"<|start_header_id|>assistant<|end_header_id|>\\n\\nLabeled Category:\")[0].split(\"<|eot_id|>\")[0].strip() for d in decode_batch]\n",
        "  # print(labels)\n",
        "  # labels  = [d.split(\"<|start_header_id|>assistant<|end_header_id|>\\n\\nLabeled Category:\")[1].split(\"<|eot_id|>\")[0].strip() for d in decode_batch]\n",
        "  return labels"
      ],
      "metadata": {
        "id": "ovenKje4g8bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_outputs(prompts, model, tokenizer):\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "  tokenized = tokenizer(prompts, padding= True, return_tensors =\"pt\", add_special_tokens= False).to(device)\n",
        "\n",
        "  # tokenized = tokenizer.apply_chat_template(prompts, padding= False, return_tensors =\"pt\", add_special_tokens= False).to(device)\n",
        "  output_batch = peft_model.generate(input_ids = tokenized[\"input_ids\"], attention_mask =tokenized[\"attention_mask\"], max_new_tokens = 20, do_sample= False, temperature = 0, top_p =1)\n",
        "  # output_batch = model.generate(tokenized, max_new_tokens = 20)\n",
        "  decode_batch = tokenizer.batch_decode(output_batch, skip_special_tokens= True)\n",
        "  prediction = extract_labels_from_output(decode_batch, tokenizer)\n",
        "  return prediction"
      ],
      "metadata": {
        "id": "Eq-3xtojs2-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(dataloader, model, tokenizer):\n",
        "  comparison_df= {\n",
        "      \"predictions\": [],\n",
        "      \"labels\": []\n",
        "  }\n",
        "\n",
        "  for batch in dataloader:\n",
        "    predictions = generate_outputs(prompts= batch[\"prompt\"], model = model, tokenizer= tokenizer) # prompts = batch[\"prompt\"]\n",
        "    comparison_df[\"labels\"].extend(batch[\"Category\"])\n",
        "    comparison_df[\"predictions\"].extend(predictions)\n",
        "\n",
        "  comparison_df = pd.DataFrame(comparison_df)\n",
        "  accuracy = (comparison_df[\"labels\"] == comparison_df[\"predictions\"]).mean()\n",
        "  num_invalid_pred = (~comparison_df[\"predictions\"].isin(valid_category)).mean()\n",
        "  print(comparison_df.head(10))\n",
        "  return {\"accuracy \": accuracy,\"invalid_predictions\": num_invalid_pred}\n",
        "  # return comparison_df"
      ],
      "metadata": {
        "id": "CoN4WQ7rsn9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # from datasets import Dataset #defined by huggingface not pandas or tf\n",
        "test_Dataset = Dataset.from_pandas(df_train[10:25]) # testing for trained data\n",
        "test_dataloader = DataLoader(test_Dataset, shuffle= False)\n",
        "metrics = test_model(test_dataloader, model, tokenizer)\n",
        "print(\"\\n\".join([f\"{k} = {v}\" for k, v in metrics.items()]))"
      ],
      "metadata": {
        "id": "lYjkUmGirsuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jgt_ZuIq3yNi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}