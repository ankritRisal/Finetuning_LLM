{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNkTgR+vMsE805uRRmAPMMU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankritRisal/Finetuning_LLM/blob/main/Train_finetunned_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Play with data\n",
        "\n"
      ],
      "metadata": {
        "id": "cnuW71GTNW6m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiKbjTf5NBO-"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "PQDmCG6KN5vD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Get the token from Colab secrets\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "login(token=hf_token)\n"
      ],
      "metadata": {
        "id": "Yb2-S9QjcviM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side =\"left\")\n",
        "tokenizer.pad_token = tokenizer.eos_token # padding tokens to make of same shape\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                             dtype = torch.bfloat16,\n",
        "                                             device_map = device)"
      ],
      "metadata": {
        "id": "Rw2uACxhfzmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization of prediction"
      ],
      "metadata": {
        "id": "F-6Be35wuRsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WHAT DO BATCH PROMPT SHOULD INCLUDE ? => CHAT_TEMPLATE + \" \" + CATEGORY_TEMPLATE\n",
        "# CATEGORY TEMPLATE => TITLE AND DESCRIPTION WITH VALID CATEGORY AS ANSWER (SHOULD OPERATE IN LOOP)\n",
        "#   SYSTEM PROMPT , PORT PROMPT > CATEGORY TEMPLATE"
      ],
      "metadata": {
        "id": "jF0W53R3zBdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/X3s4c5/FinetunningLLMmodels/book_description.csv\")\n",
        "df.drop([\"Unnamed: 0\", \"Unnamed: 0.1\", \"Price\", \"Avilability\", \"Stars\", ], axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "UHypglYKyuyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_prompt(row):\n",
        "    valid_category = list(df[\"Category\"].unique())\n",
        "    SYSTEM_PROMPT =  \\\n",
        "      {\n",
        "        \"role\" : \"system\",\n",
        "        \"content\" : f\"\"\" You are an AI system that reads an Title and Book Description and classifies category of the book applied, you must\n",
        "        choose from the following classes:\n",
        "        {\"\\n or \". join([\"Labeled Category:\" + x for x in list(valid_category)])}.\n",
        "        Ensure Output is from above list only\"\"\"\n",
        "        }\n",
        "\n",
        "    ASSISTANT_MESSAGES = \\\n",
        "      {\n",
        "        \"role\" : \"assistant\",\n",
        "        \"content\" : \"Labeled Category :\"\n",
        "        }\n",
        "\n",
        "    USER_MESSAGES = {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"\"\"Title: {row['Title']}\n",
        "Description: {row['Book_Description']}\"\"\"\n",
        "    }\n",
        "\n",
        "    prompt = [\n",
        "        SYSTEM_PROMPT,\n",
        "        USER_MESSAGES,\n",
        "        ASSISTANT_MESSAGES\n",
        "    ]\n",
        "    # print(prompt)\n",
        "    tokenized_prompt = tokenizer.apply_chat_template(prompt, continue_final_message= True, tokenize= False)\n",
        "    return tokenized_prompt"
      ],
      "metadata": {
        "id": "nPNPGJIGoPUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_seed = 32\n",
        "train_size = 0.95\n",
        "df[\"prompt\"] = df.apply(build_prompt, axis=1)\n",
        "df = df.sample(frac=1, random_state = random_seed).reset_index(drop=True).reset_index()\n",
        "train_len = int(train_size * len(df))\n",
        "df_train= df[:train_len]\n",
        "df_test = df[train_len:]"
      ],
      "metadata": {
        "id": "H5uzUcDSoaQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_dataset(tokenizer, train_size, build_prompt):\n",
        "#   df = pd.read_csv(\"/content/drive/MyDrive/X3s4c5/FinetunningLLMmodels/book_description.csv\")\n",
        "\n",
        "#   df.drop([\"Unnamed: 0\", \"Unnamed: 0.1\", \"Price\", \"Avilability\", \"Stars\", ], axis = 1, inplace = True)\n",
        "\n",
        "#   df[\"prompt\"] = df.apply(build_prompt, axis=1)\n",
        "#   df = df.sample(frac=1, random_state = random_seed).reset_index(drop=True).reset_index()\n",
        "#   train_len = int(train_size * len(df))\n",
        "#   df_train= df[:train_len]\n",
        "#   df_test = df[train_len:]\n",
        "#   # print(df_test)\n",
        "#   return df_train, df_test"
      ],
      "metadata": {
        "id": "ggxWFPRIyip1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_labels_from_output(decode_batch, tokenizer):\n",
        "  labels = []\n",
        "  for d in decode_batch:\n",
        "    # print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
        "    # print(d)\n",
        "    label = d.split(\"Labeled Category :\")[1].strip()\n",
        "    # print(label)\n",
        "    labels.append(label)\n",
        "  # labels = [d.split(\"<|start_header_id|>assistant<|end_header_id|>\\n\\nLabeled Category:\")[0].split(\"<|eot_id|>\")[0].strip() for d in decode_batch]\n",
        "  # print(labels)\n",
        "  # labels  = [d.split(\"<|start_header_id|>assistant<|end_header_id|>\\n\\nLabeled Category:\")[1].split(\"<|eot_id|>\")[0].strip() for d in decode_batch]\n",
        "  return labels"
      ],
      "metadata": {
        "id": "8F53QY_yuvDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_outputs(prompts, model, tokenizer):\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "  tokenized = tokenizer(prompts, padding= True, return_tensors =\"pt\", add_special_tokens= False).to(device)\n",
        "\n",
        "  # tokenized = tokenizer.apply_chat_template(prompts, padding= False, return_tensors =\"pt\", add_special_tokens= False).to(device)\n",
        "  output_batch = model.generate(input_ids = tokenized[\"input_ids\"], attention_mask =tokenized[\"attention_mask\"], max_new_tokens = 20, do_sample= False, temperature = 0, top_p =1)\n",
        "  # output_batch = model.generate(tokenized, max_new_tokens = 20)\n",
        "  decode_batch = tokenizer.batch_decode(output_batch, skip_special_tokens= True)\n",
        "  prediction = extract_labels_from_output(decode_batch, tokenizer)\n",
        "  return prediction"
      ],
      "metadata": {
        "id": "Eq-3xtojs2-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(dataloader, model, tokenizer):\n",
        "  comparison_df= {\n",
        "      \"predictions\": [],\n",
        "      \"labels\": []\n",
        "  }\n",
        "\n",
        "  for batch in dataloader:\n",
        "    predictions = generate_outputs(prompts= batch[\"prompt\"], model = model, tokenizer= tokenizer) # prompts = batch[\"prompt\"]\n",
        "    comparison_df[\"labels\"].extend(batch[\"Category\"])\n",
        "    comparison_df[\"predictions\"].extend(predictions)\n",
        "\n",
        "  # comparison_df = pd.DataFrame(comparison_df)\n",
        "  # accuracy = (comparison_df[\"labels\"] == comparison_df[\"predictions\"]).mean()\n",
        "  # num_invalid_pred = (~comparison_df[\"predictions\"].isin(valid_category)).mean()\n",
        "  # print(comparison_df.head(10))\n",
        "  # return {\"accuracy \": accuracy,\"invalid_predictions\": num_invalid_pred}\n",
        "  return comparison_df"
      ],
      "metadata": {
        "id": "CoN4WQ7rsn9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset #defined by huggingface not pandas or tf\n",
        "batch_size = 16\n",
        "test_Dataset = Dataset.from_pandas(df_test)\n",
        "test_dataloader = DataLoader(test_Dataset, batch_size = batch_size, shuffle= False)\n",
        "metrics = test_model(test_dataloader, model, tokenizer)\n",
        "# print(\"\\n\".join([f\"{k} = {v}\" for k, v in metrics.items()]))"
      ],
      "metadata": {
        "id": "lYjkUmGirsuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_Dataset"
      ],
      "metadata": {
        "id": "zFQVTxOzvXkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_category = list(df[\"Category\"].unique())\n",
        "comparison = pd.DataFrame(metrics)\n",
        "accuracy = (comparison[\"labels\"] == comparison[\"predictions\"]).mean()\n",
        "num_invalid_pred = (~comparison[\"predictions\"].isin(valid_category)).mean()\n",
        "print(comparison.head(10))\n",
        "print(f\"accuracy : {accuracy}, invalid_predictions: {num_invalid_pred}\")"
      ],
      "metadata": {
        "id": "oXhLng9Is2Qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title_name = \"Ouran High School Host Club, Vol. 1 (Ouran High School Host Club #1)\"\n",
        "df[df[\"Title\"] == title_name]"
      ],
      "metadata": {
        "id": "sfZ1QEcW188G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dataloader is object but we need list"
      ],
      "metadata": {
        "id": "498nIF-SGkzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2758,\n",
        "#            264,   4068,"
      ],
      "metadata": {
        "id": "3CCd9rwb7T84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.convert_ids_to_tokens(2758))\n",
        "# print(tokenizer.convert_tokens_to_ids(\"user\"))\n",
        "print(tokenizer.convert_ids_to_tokens(264))\n",
        "print(tokenizer.convert_ids_to_tokens(4068))\n"
      ],
      "metadata": {
        "id": "E5yfb2uD4XmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_input_output_pair(batch_prompts, batch_targets):\n",
        "\n",
        "  full_response_text = [\n",
        "      (b_prompt + \" \" + target + tokenizer.eos_token)\n",
        "      for b_prompt, target in zip(batch_prompts, batch_targets)\n",
        "      ]\n",
        "  input_ids_tokenized = tokenizer(full_response_text, add_special_tokens = False, return_tensors =\"pt\", padding =True)[\"input_ids\"]\n",
        "  label_tokenized = tokenizer([\" \" + target + tokenizer.eos_token for target in batch_targets], add_special_tokens = False,\n",
        "                              return_tensors =\"pt\", padding = \"max_length\", max_length = input_ids_tokenized.shape[1])[\"input_ids\"]\n",
        "\n",
        "  label_tokenized_fixed = torch.where(label_tokenized != tokenizer.pad_token_id, label_tokenized, -100)\n",
        "  label_tokenized_fixed[:, -1] = tokenizer.eos_token_id\n",
        "\n",
        "  input_ids_tokenized_left_shifted = input_ids_tokenized[:, :-1]\n",
        "  label_tokenized_right_shifted = label_tokenized_fixed[:, 1:]\n",
        "\n",
        "  attention_mask = input_ids_tokenized_left_shifted != tokenizer.pad_token_id\n",
        "\n",
        "  return {\n",
        "      \"input_ids\" : input_ids_tokenized_left_shifted,\n",
        "      \"attention_mask\" : attention_mask,\n",
        "      \"labels\" : label_tokenized_right_shifted\n",
        "  }"
      ],
      "metadata": {
        "id": "bzt9JcoG9UtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "\n",
        "def calculate_loss(logits, labels):\n",
        "  loss_fn = nn.CrossEntropyLoss(reduction= 'none')\n",
        "  entropyloss = loss_fn(logits.view(-1, logits.size(-1)), labels.reshape(-1))\n",
        "  return entropyloss"
      ],
      "metadata": {
        "id": "8KXdyjoBDKJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LORA ADAPTOR**"
      ],
      "metadata": {
        "id": "Hfo5B35cIrWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LORA ADAPTOR\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    task_type = \"CAUSAL_LM\",\n",
        "    r = 4,\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0.05,\n",
        "    target_modules = ['q_proj', 'v_proj']\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "CHWvVt_4IJEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DEFINE TRAINING LOOP**"
      ],
      "metadata": {
        "id": "1AjxDPw6z8cX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DURING TRAINING: TOKENIZED VALUE MUST BE COMPARED WITH LABELED VALUE\n",
        "# HOW TO DO IT ?\n",
        "# USE NEW DF PROMPT FOR EACH ONE"
      ],
      "metadata": {
        "id": "gQP6649_0bPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset #defined by huggingface not pandas or tf\n",
        "batch_size = 16\n",
        "train_Dataset = Dataset.from_pandas(df_train)\n",
        "train_dataloader = DataLoader(train_Dataset, batch_size = batch_size, shuffle= False)"
      ],
      "metadata": {
        "id": "68nkqd5HHRUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_dataloader :\n",
        "  data = generate_input_output_pair(batch_prompts= batch['prompt'], batch_targets= batch['Category'])\n",
        "  out = model(input_ids = data[\"input_ids\"].to(device))\n",
        "  loss = calculate_loss(out.logits, data[\"labels\"].to(device))"
      ],
      "metadata": {
        "id": "Ni-OeMW7ISVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1\n",
        "optimizer = AdamW(model.parameters(), lr = 1e-3, weight_decay= 0.01)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  running_loss = 0.0\n",
        "\n",
        "  for batch in train_dataloader :\n",
        "    data = generate_input_output_pair(batch_prompts= batch['prompt'], batch_targets= batch['Category'])\n",
        "    out = model(input_ids = data[\"input_ids\"].to(device))\n",
        "    loss = calculate_loss(out.logits, data[\"labels\"].to(device))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "    running_loss += loss.item()\n",
        "\n",
        "  avg_loss = running_loss / len(train_dataloader)"
      ],
      "metadata": {
        "id": "wBtzyvP-CY7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in test_dataloader:\n",
        "  data = generate_input_output_pair(batch_prompts= batch['prompt'], batch_targets= batch['Category'])\n",
        "  data[\"input_ids\"]"
      ],
      "metadata": {
        "id": "em__x9NP-B85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[\"input_ids\"].shape)\n",
        "print(data[\"labels\"].shape)"
      ],
      "metadata": {
        "id": "zaYxoGC8-YJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss = calculate_loss(out.logits, data[\"labels\"].to(device))"
      ],
      "metadata": {
        "id": "aPE0aYf3ASik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(dataloader, model, tokenizer):\n",
        "  for batch in dataloader:\n",
        "    data = generate_input_output_pair(prompt= batch['prompt'], target_responses= batch['Category'])"
      ],
      "metadata": {
        "id": "VXPsyvF98YYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vgwm33jGfzc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aG3Aqgsrfzaf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}